{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subcellular location prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r_tvA4BYVwPe",
        "XmdrKoT1Ucv7",
        "OYiCUqVqUg4x",
        "UH2-oxWMUg7V",
        "E-6zc-WqVEH8",
        "y6QzHNWbWzA4",
        "zTR-sNUkW5E9",
        "XIrNCH7eXBBZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tmoKtlLUOna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSBeqCx-UW2R",
        "colab_type": "text"
      },
      "source": [
        "#Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWlOjLJnUTc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import pandas as pd\n",
        "from torch.autograd import Variable\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v69w9qHMUaVz",
        "colab_type": "text"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b26eLanUTfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_enc_length = 2000     #99% of the data sequences are below this limit\n",
        "\n",
        "data_path = 'drive/My Drive/Bioinformatic'\n",
        "classes = ['cyto',  'secreted','mito', 'nucleus']\n",
        "test_name = 'blind'\n",
        "pkl_path = 'drive/My Drive/Bioinformatic/pkl'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_tvA4BYVwPe",
        "colab_type": "text"
      },
      "source": [
        "#Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmdrKoT1Ucv7",
        "colab_type": "text"
      },
      "source": [
        "##Load .fasta data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKx75u7RUTiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _parse_fasta(data, lines, c):\n",
        "    lines_iter = iter(lines)\n",
        "    line = next(lines_iter, None)       #return None if there is no more items\n",
        "    max_len = 0\n",
        "    while line:\n",
        "        assert line.startswith('>')\n",
        "        info = line\n",
        "        line = next(lines_iter, None)\n",
        "        seq = ''\n",
        "        while line and not line.startswith('>'):\n",
        "            seq += line.rstrip()\n",
        "            line = next(lines_iter, None)\n",
        "        max_len = max((max_len, len(seq)))\n",
        "        data['info'].append(info)\n",
        "        data['seq'].append(seq)\n",
        "        data['class'].append(c)\n",
        "    return max_len\n",
        "\n",
        "def _load_data(data, name, c):\n",
        "    with open('{}/{}.fasta'.format(data_path, name), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return _parse_fasta(data, lines, c)\n",
        "\n",
        "\n",
        "def _get_data():\n",
        "    train = {'info': [], 'seq': [], 'class': []}\n",
        "    test = {'info': [], 'seq': [], 'class': []}\n",
        "    max_len = _load_data(test, test_name, None)\n",
        "    print('Max sequence length test: {}'.format(max_len))\n",
        "    for c in classes:\n",
        "        seq_len = _load_data(train, c, c)\n",
        "        max_len = max((max_len, seq_len))\n",
        "    print('Max sequence length: {}'.format(max_len))\n",
        "    print('Train sequences: {}'.format(len(train['info'])))\n",
        "    print('Test sequences: {}'.format(len(test['info'])))\n",
        "    return train, test, max_len\n",
        "\n",
        "train, test, max_len = _get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYiCUqVqUg4x",
        "colab_type": "text"
      },
      "source": [
        "##Build dictionary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrOmUYv7UsUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def build_dictionary(sentences, vocab=None, max_sent_len_=None):            #use later\n",
        "\n",
        "    '''\n",
        "    This function takes as input raw amino acid sequences and outputs\n",
        "        - a dictionary and reverse ditionary containing all tokens (amino acids)\n",
        "        - encoded amino acid sequences. This is because the RNN requires numeric inputs\n",
        "        - the sequence length for each protein and the max sequence length across all proteins.\n",
        "            This is needed for the dynamic RNN function in tf.\n",
        "    If no vocabulary is provided, a new one will be create (this is used on the training data)\n",
        "    If a vocabulary is provided, then this vocabulary will be used to encode sequences (this is used on the test data)\n",
        "    '''\n",
        "    is_ext_vocab = True\n",
        "\n",
        "    # If no vocab provided, create a new one\n",
        "    if vocab is None:\n",
        "        is_ext_vocab = False\n",
        "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
        "\n",
        "    # Create list that will contain integer encoded senteces \n",
        "    data_sentences = []\n",
        "    max_sent_len = -1\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = []\n",
        "        for word in sentence:\n",
        "            # If creating a new vocab, and word isnt in vocab yet, add it\n",
        "            if not is_ext_vocab and word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "            # Now add either OOV or actual token to words\n",
        "            if word not in vocab:\n",
        "                token_id = vocab['<OOV>']       #word is not in the vocab\n",
        "            else:\n",
        "                token_id = vocab[word]\n",
        "            words.append(token_id)\n",
        "        if len(words) > max_sent_len:\n",
        "            max_sent_len = len(words)\n",
        "        data_sentences.append(words)\n",
        "\n",
        "    if max_sent_len_ is not None:\n",
        "        max_sent_len = max_sent_len_            #if max length has been setup before\n",
        "\n",
        "    enc_sentences = np.full([len(data_sentences), max_sent_len], vocab['<PAD>'], dtype=np.int32)        #matrix full with id of <PAD>\n",
        "\n",
        "    sentence_lengths = []\n",
        "    for i, sentence in enumerate(data_sentences):\n",
        "        enc_sentences[i, 0:len(sentence)] = sentence\n",
        "        sentence_lengths.append(len(sentence))\n",
        "\n",
        "    sentence_lengths = np.array(sentence_lengths, dtype=np.int32)\n",
        "    reverse_dictionary = dict(zip(vocab.values(), vocab.keys()))\n",
        "\n",
        "    return vocab, reverse_dictionary, sentence_lengths, max_sent_len+1, enc_sentences\n",
        "\n",
        "\n",
        "\n",
        "def generate_pkl(train):\n",
        "\n",
        "    '''\n",
        "    This takes as input a train/test flag, and then takes the raw \n",
        "        input files in fasta format and return pickle files.\n",
        "    '''\n",
        "    if train:\n",
        "        files = ['cyto.fasta', 'secreted.fasta', 'mito.fasta', 'nucleus.fasta']\n",
        "        prefix = 'train_'\n",
        "    else:\n",
        "        files = ['blind.fasta']\n",
        "        prefix = 'test_'\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    s = ''\n",
        "    for fasta_file in files:                    #read each data file seperately\n",
        "        with open('{}/{}'.format(data_path,fasta_file), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        count = 0\n",
        "        for l in lines:\n",
        "            if l[0] == '>':\n",
        "                if count > 0:\n",
        "                    data.append(s)\n",
        "                    labels.append(files.index(fasta_file))         #label list ----0: cyto; 1: secreted;  2: mito; 3: nucleus\n",
        "                s = ''\n",
        "            else:\n",
        "                s += l[:-1]\n",
        "                if count == len(lines) - 1:\n",
        "                    data.append(s)\n",
        "                    labels.append(files.index(fasta_file))\n",
        "            count += 1\n",
        "    with open('{}/{}'.format(pkl_path,prefix + 'data.pkl'), 'wb') as f:\n",
        "        pkl.dump(data, f)\n",
        "    with open('{}/{}'.format(pkl_path,prefix + 'labels.pkl'), 'wb') as f:\n",
        "        pkl.dump(labels, f)\n",
        "\n",
        "\n",
        "def get_data(train):\n",
        "    if train:\n",
        "        prefix = 'train_'\n",
        "    else:\n",
        "        prefix = 'test_'\n",
        "    if not os.path.exists(prefix + 'data.pkl'):\n",
        "        print('Generating data')\n",
        "        generate_pkl(train)\n",
        "    with open('{}/{}'.format(pkl_path,prefix + 'data.pkl'), 'rb') as f:\n",
        "        data = pkl.load(f)\n",
        "    with open('{}/{}'.format(pkl_path,prefix + 'labels.pkl'), 'rb') as f:\n",
        "        labels = pkl.load(f)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQlWIgELUxch",
        "colab_type": "text"
      },
      "source": [
        "##Save pickle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpu39z84UzDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Data, Labels = get_data(train = True)           #list with len = 9222"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24iYQYGVU1Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data, test_labels = get_data(train = False)        #testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH2-oxWMUg7V",
        "colab_type": "text"
      },
      "source": [
        "##Load pickle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFd809edU98D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('{}/{}'.format(pkl_path,'train_' + 'data.pkl'), 'rb') as f:\n",
        "    Data = pkl.load(f)\n",
        "with open('{}/{}'.format(pkl_path,'train_' + 'labels.pkl'), 'rb') as f:\n",
        "    Labels = pkl.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzLN9AXeU_ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('{}/{}'.format(pkl_path, 'test_' + 'data.pkl'), 'rb') as f:\n",
        "    test_data = pkl.load(f)\n",
        "    #no label for testing data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJyBEAl8Ug-B",
        "colab_type": "text"
      },
      "source": [
        "##Dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyZ3VCHFVCUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab, reverse_dictionary, sentence_lengths, max_sent_len, enc_sentences = build_dictionary(Data)       #training data, the enc_sentence has (9222,13100)\n",
        "test_vocab, test_reverse, test_sentence_lengths, test_max_sent_len, test_enc_sentences = build_dictionary(test_data, vocab=vocab, max_sent_len_= max_enc_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-6zc-WqVEH8",
        "colab_type": "text"
      },
      "source": [
        "##Statistics of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc7uxuB9VRMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def summary_stats(lengths, labels, name):\n",
        "\n",
        "    '''\n",
        "    Takes as input the lengths and labels of amino acid sequences\n",
        "    Prints and returns pandas dataframes containing descriptive statistics\n",
        "    '''\n",
        "\n",
        "    bins = [0,100,500,1000,1500,2000,2499]\n",
        "    labels_string = ['cyto', 'secreted', 'mito', 'nucleus']\n",
        "        \n",
        "    df = pd.DataFrame({'length': lengths, 'label': labels})\n",
        "    table = pd.crosstab(np.digitize(df.length, bins), df.label)\n",
        "    table.index = pd.Index(['[0, 100)', '[100, 500)', '[500, 1000]', '[1000, 1500)', '[1500, 2000)','[2000, 2500]', '[2500, inf]'], name=\"Bin\")\n",
        "    table.columns = pd.Index(labels_string, name=\"Class\")\n",
        "        \n",
        "    sum_row = {col: table[col].sum() for col in table}\n",
        "    sum_df = pd.DataFrame(sum_row, index=[\"Total\"])\n",
        "    table = table.append(sum_df)\n",
        "    table['Total'] = table.sum(axis=1)\n",
        "\n",
        "    print('\\n~~~~~~~ Summary stats for %s set ~~~~~~~' % name)\n",
        "    print('\\nCount of sequence lengths by class')\n",
        "    print(table)\n",
        "    print('\\nDescriptive statistics')\n",
        "    print(df.describe())\n",
        "\n",
        "    return df, table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrjUOT7nVRQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(test_sentence_lengths, bins = 20, range=(0,2000));\n",
        "plt.xlabel('Length of sequences');\n",
        "plt.ylabel('Frequency');\n",
        "plt.title('Histogram of testing sequence length');\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn7Nk_ZDVRRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_statistics, train_frame = summary_stats(sentence_lengths,np.array(Labels) ,'train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XikF55ihVdiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_statistics, valid_frame = summary_stats(enc_valid_length, enc_valid_label, 'validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZp3G4BDVp8F",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwviDhPIWKe6",
        "colab_type": "text"
      },
      "source": [
        "##LSTM, attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC1lGk45VqyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class attentionLSTM(nn.Module):\n",
        "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_size, bidirection, if_attention):\n",
        "        super(attentionLSTM, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.bidirection = bidirection\n",
        "        self.if_attention = if_attention\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "\n",
        "        self.h_0 = nn.Parameter(T.rand(2 if self.bidirection else 1, self.batch_size, self.hidden_size).type(T.FloatTensor), requires_grad = True)  #learn initial state\n",
        "        self.c_0 = nn.Parameter(T.rand(2 if self.bidirection else 1, self.batch_size, self.hidden_size).type(T.FloatTensor), requires_grad = True)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, batch_first = False, bidirectional = self.bidirection)\n",
        "        self.linear1 = nn.Linear(self.hidden_size * (2 if self.bidirection else 1) , self.hidden_size )               #since we do for each batch data?\n",
        "        #self.linear2 = nn.Linear(max_enc_length , self.output_size)\n",
        "        self.linear2 = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.att_wh = nn.Linear(self.hidden_size * (2 if self.bidirection else 1), self.hidden_size * (2 if self.bidirection else 1), bias=False)\n",
        "        self.att_ws = nn.Linear(self.hidden_size * (2 if self.bidirection else 1), self.hidden_size * (2 if self.bidirection else 1))\n",
        "        self.att_v = nn.Linear(self.hidden_size * (2 if self.bidirection else 1), 1, bias = False)\n",
        "\n",
        "    def attention_net(self, output_lstm, last_hidden, enc_padding_mask):\n",
        "        \"\"\"\n",
        "        the attention of each of the hidden state of lstm and the last hidden state\n",
        "\n",
        "        param: last_hidden : [num_direction, batch_size, hidden_size]\n",
        "        param: output_lstm : [batch_size, seq_length(2500), num_direction * hidden_size]\n",
        "        param: enc_padding_mask: [batch_size, seq_length(2500)], \n",
        "        \"\"\"\n",
        "        #hidden = last_hidden#.squeeze(0)                     #[batch_size, hidden_size]\n",
        "        #hidden = last_hidden.view(last_hidden.shape[1], -1)\n",
        "\n",
        "        #attention_weight = T.bmm(output_lstm, hidden.unsqueeze(2)).squeeze(2)           #[batch_size, seq_length], maybe do 0out probability of padded tokens??\n",
        "        #attention_weight = attention_weight * enc_padding_mask\n",
        "        #normalisation_factor = attention_weight.sum(1, keepdim = True)\n",
        "        #attention_weight = attention_weight/normalisation_factor\n",
        "        \n",
        "        #softmax_att_weight = F.softmax(attention_weight,dim = 1)        #can try linear normalisation, if we zero out padding then cannot use softmax anymore\n",
        "        \n",
        "        #new_hidden_state = T.bmm(output_lstm.transpose(1,2), attention_weight.unsqueeze(2)).squeeze(2)        #[batch_size, num_direction * hidden_size]\n",
        "        et = self.att_wh(output_lstm)           #[batch_size, seq_length, num_direction*hidden_size]\n",
        "        hidden = last_hidden.view(last_hidden.shape[1], -1)         #[batch_size, num_di*hidden_size]\n",
        "        final_fea = self.att_ws(hidden).unsqueeze(1)                #[batch_size, 1, num_di*hidden_size]\n",
        "        et = et + final_fea\n",
        "        et = T.tanh(et)\n",
        "        et = self.att_v(et).squeeze(2)          #[batch_size, seq_length]\n",
        "\n",
        "        et1 = F.softmax(et, dim = 1)\n",
        "        at = et1 * enc_padding_mask\n",
        "        normalisation_factor = at.sum(1, keepdim = True)\n",
        "        at = at/normalisation_factor\n",
        "\n",
        "        at = at.unsqueeze(1)            #[batch_size, 1, seq_length]\n",
        "        ct_e = T.bmm(at, output_lstm)      #[batch_size, 1, num_direction * hidden_size]\n",
        "        ct_e = ct_e.squeeze(1)\n",
        "        at = at.squeeze(1)\n",
        "        return ct_e, at    #(batch_size, num_direction * hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self,input_sequence, enc_padding_mask,batch_size_setup = None):\n",
        "\n",
        "        embedded = self.embedding(input_sequence)   #input_seq [batch_size, seq_length]; embedded [batch_size, seq_length, embedding_size]\n",
        "        embedded = embedded.permute(1,0,2)          #so that no need to batch_first;  [seq_length, batch_size, embedding_size]  https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402\n",
        "\n",
        "        output, (final_hidden_state, final_cell_state) = self.lstm(embedded, (self.h_0, self.c_0))  #output [seq_length, batch, nu_direction*hidden_szie]\n",
        "        output = output.permute(1,0,2)                          #[batch_size, seq_length, num_di*hidden_size]          \n",
        "\n",
        "        if self.if_attention:\n",
        "            attention_output, attention_weight= self.attention_net(output, final_hidden_state, enc_padding_mask)       #[batch_size, num_direction * hidden_size]\n",
        "            output = self.linear1(attention_output)             #[batch_size,  hidden_size]\n",
        "            output = T.tanh(output)\n",
        "            logits = F.softmax(self.linear2(output),dim=1)\n",
        "        else:      #no attention\n",
        "            output = output.permute(0, 2, 1)            #[batch_size, num_di*hidden_size, seq_length]\n",
        "            output = F.max_pool1d(output, output.shape[2]).squeeze(2)   #[batch_size, num_di*hidden_size],  choose maximum value at each hidden entry\n",
        "            output = self.linear1(T.tanh(output))\n",
        "            logits = F.softmax(self.linear2(output), dim = 1)       # MLP\n",
        "\n",
        "            #output = self.linear1(final_hidden_state.squeeze(1))\n",
        "            #output = T.tanh(output)\n",
        "            #output = self.linear2(output)\n",
        "            #logits = F.softmax(output, dim = 1)\n",
        "\n",
        "            \"\"\"\n",
        "            output = T.tanh(output)                                         #[batch_size, seq_length, num_direction*hidden_size]\n",
        "            output = F.max_pool1d(output, output.shape[2]).squeeze(2)       #[batch_size, seq_length]\n",
        "            output = T.tanh(output)\n",
        "            output = output * enc_padding_mask                              #[batch_size, seq_length]\n",
        "            logits = F.softmax(self.linear2(output), dim = 1)                #[batch_size, output_size]\n",
        "            \"\"\"\n",
        "            #logits = F.softmax(self.linear(final_hidden_state[-1]), dim = 1)\n",
        "            attention_weight = 0\n",
        "\n",
        "        return logits, attention_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eksbhyEWNS8",
        "colab_type": "text"
      },
      "source": [
        "##LSTM, CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpM0iBXNVq2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_lstm(nn.Module):\n",
        "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_size, kernel_size,bidirection, if_attention):\n",
        "        super(CNN_lstm, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_kernel = len(kernel_size)\n",
        "        self.bidirection = bidirection\n",
        "        self.if_attention = if_attention\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "\n",
        "        KK = []\n",
        "        for k in kernel_size:\n",
        "            KK.append( k + 1 if k % 2 == 0 else k)\n",
        "        self.conv = [get_cuda(nn.Conv2d(1, self.embedding_size, (k, self.embedding_size), stride = 1, padding = (k // 2, 0))) for k in KK]\n",
        "\n",
        "        #self.conv = nn.Conv2d(1, self.embedding_size, (self.kernel_size, self.embedding_size), stride=1, padding=(1, 0))\n",
        "\n",
        "        self.h_0 = nn.Parameter(T.rand(2 if self.bidirection else 1, self.batch_size, self.hidden_size).type(T.FloatTensor), requires_grad = True)  #learn initial state\n",
        "        self.c_0 = nn.Parameter(T.rand(2 if self.bidirection else 1, self.batch_size, self.hidden_size).type(T.FloatTensor), requires_grad = True)\n",
        "        self.lstm = nn.LSTM(self.embedding_size * self.num_kernel, self.hidden_size, batch_first = False, bidirectional = self.bidirection)\n",
        "        self.linear1 = nn.Linear(self.hidden_size * (2 if self.bidirection else 1) , (self.hidden_size * (2 if self.bidirection else 1))//2)               #since we do for each batch data?\n",
        "        self.linear2 = nn.Linear((self.hidden_size * (2 if self.bidirection else 1))//2, self.output_size )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear2 = nn.Linear(max_enc_length , self.output_size)\n",
        "\n",
        "        self.att_wh = nn.Linear(self.hidden_size * (2 if self.bidirection else 1), self.hidden_size * (2 if self.bidirection else 1), bias=False)\n",
        "        self.att_ws = nn.Linear(self.hidden_size * (2 if self.bidirection else 1), self.hidden_size * (2 if self.bidirection else 1))\n",
        "        self.att_v = nn.Linear(self.hidden_size * (2 if self.bidirection else 1), 1, bias = False)\n",
        "    \n",
        "    def attention_net(self, output_lstm, last_hidden, enc_padding_mask):\n",
        "        \"\"\"\n",
        "        the attention of each of the hidden state of lstm and the last hidden state\n",
        "\n",
        "        param: last_hidden : [num_direction, batch_size, hidden_size]\n",
        "        param: output_lstm : [batch_size, seq_length(2500), num_direction * hidden_size]\n",
        "        param: enc_padding_mask: [batch_size, seq_length(2500)], \n",
        "        \"\"\"\n",
        "        et = self.att_wh(output_lstm)           #[batch_size, seq_length, num_direction*hidden_size]\n",
        "        hidden = last_hidden.view(last_hidden.shape[1], -1)         #[batch_size, num_di*hidden_size]\n",
        "        final_fea = self.att_ws(hidden).unsqueeze(1)                #[batch_size, 1, num_di*hidden_size]\n",
        "        et = et + final_fea\n",
        "        et = T.tanh(et)\n",
        "        et = self.att_v(et).squeeze(2)          #[batch_size, seq_length]\n",
        "\n",
        "        et1 = F.softmax(et, dim = 1)\n",
        "        at = et1 * enc_padding_mask\n",
        "        normalisation_factor = at.sum(1, keepdim = True)\n",
        "        at = at/normalisation_factor\n",
        "\n",
        "        at = at.unsqueeze(1)            #[batch_size, 1, seq_length]\n",
        "        ct_e = T.bmm(at, output_lstm)      #[batch_size, 1, num_direction * hidden_size]\n",
        "        ct_e = ct_e.squeeze(1)\n",
        "        at = at.squeeze(1)\n",
        "\n",
        "        return ct_e, at\n",
        "\n",
        "        \"\"\"\n",
        "        hidden = last_hidden.view(last_hidden.shape[1], -1)\n",
        "        print('hidden size', hidden.size())\n",
        "\n",
        "        attention_weight = T.bmm(output_lstm, hidden.unsqueeze(2)).squeeze(2)           #[batch_size, seq_length], maybe do 0out probability of padded tokens??\n",
        "        print('attention weight size', attention_weight.size())\n",
        "        attention_weight = attention_weight * enc_padding_mask\n",
        "        normalisation_factor = attention_weight.sum(1, keepdim = True)\n",
        "        attention_weight = attention_weight/normalisation_factor\n",
        "        \n",
        "        #softmax_att_weight = F.softmax(attention_weight,dim = 1)        #can try linear normalisation, if we zero out padding then cannot use softmax anymore\n",
        "        \n",
        "        new_hidden_state = T.bmm(output_lstm.transpose(1,2), attention_weight.unsqueeze(2)).squeeze(2)        #[batch_size, num_direction * hidden_size]\n",
        "\n",
        "        return new_hidden_state, attention_weight     #(batch_size, num_direction * hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, input_sequence, enc_padding_mask): \n",
        "        \"\"\"\n",
        "        input_sequence.size() :  [batch_size, seq_length]\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_sequence)        #embedded.size(): [batch_size, seq_length, embedding_size]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        conved = [self.relu(conv(embedded)).squeeze(-1) for conv in self.conv]        #each entry has [batch_size, embedding_size, seq_length]\n",
        "        conved = T.cat(conved, 1)               #concatenate, [batch_size, num_kernel*embedding_size, seq_length]\n",
        "        conved = conved.permute(2, 0, 1)        #[seq_length, batch_size, num_kernel*embedding_size]\n",
        "\n",
        "        output, (final_hidden_state, final_cell_state) = self.lstm(conved, (self.h_0, self.c_0))     #[seq_length, batch_size, num_di*hidden_size]\n",
        "        output = output.permute(1,0,2)                          #[batch_size, seq_length, num_di*hidden_size]\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        conved = self.conv(embedded.unsqueeze(1))        #conved.size():  [batch_size, embedding_size, seq_length, 1]\n",
        "        #print('size of conved 1: ', conved.size())\n",
        "        conved = conved.squeeze(-1).permute(2, 0, 1)     #                [seq_length, batch_size, embedding_size]\n",
        "\n",
        "        #print('size of conved:', conved.size())\n",
        "\n",
        "        output, (final_hidden_state, final_cell_state) = self.lstm(conved, (self.h_0, self.c_0))        #output.size():  [seq_length, batch, nu_direction*hidden_size]\n",
        "        output = output.permute(1,0,2)                  # [batch_size, seq_length, num_direction*hidden_size]\n",
        "        \"\"\"\n",
        "        if self.if_attention:\n",
        "            attention_output, attention_weight= self.attention_net(output, final_hidden_state, enc_padding_mask)       #[batch_size, num_direction * hidden_size]\n",
        "            logits = F.softmax(self.linear1(attention_output),dim=1)\n",
        "        else:      #no attention\n",
        "\n",
        "            output = output.permute(0, 2, 1)     \n",
        "            #print('ouytput size', output.size())                                   #[batch_size, num_di*hidden_size, seq_length]\n",
        "            output = F.max_pool1d(output, output.shape[2]).squeeze(2)               #[batch_size, num_di * hidden_size]\n",
        "            output = T.tanh(output)\n",
        "            output = self.linear1(output)\n",
        "            output = T.tanh(output)\n",
        "            logits = F.softmax(self.linear2(output), dim = 1)                        #this version is remove the seq_length dimension and no padding mask is involved\n",
        "\n",
        "            \"\"\"\n",
        "            output = T.tanh(output)                                         #[batch_size, seq_length, num_direction*hidden_size]\n",
        "            output = F.max_pool1d(output, output.shape[2]).squeeze(2)       #[batch_size, seq_length]\n",
        "            output = T.tanh(output)\n",
        "            output = output * enc_padding_mask                              #[batch_size, seq_length]\n",
        "            logits = F.softmax(self.linear2(output), dim = 1)                #[batch_size, output_size]\n",
        "            \"\"\"\n",
        "            #logits = F.softmax(self.linear(final_hidden_state[-1]), dim = 1)\n",
        "            attention_weight = 0\n",
        "\n",
        "        return logits, attention_weight\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV4PLsfbWThX",
        "colab_type": "text"
      },
      "source": [
        "#Run (training, validation, testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQygSS0WXFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def gradient_clamping(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "def get_cuda(tensor):\n",
        "    if T.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "def data_shuffle(enc_seq, enc_label, enc_length,random_seed = None):\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "    perm_rnd = np.random.permutation(len(enc_seq))\n",
        "    perm_seq = enc_seq[perm_rnd]\n",
        "    perm_label = enc_label[perm_rnd]\n",
        "    perm_length = enc_length[perm_rnd]\n",
        "\n",
        "    return perm_seq, perm_label, perm_length\n",
        "\n",
        "def truncate(enc_sentences, sentence_lengths):\n",
        "    original_length = copy.copy(sentence_lengths)\n",
        "    enc_truncated = enc_sentences[:,:max_enc_length]\n",
        "    for i in range(len(sentence_lengths)):\n",
        "        if sentence_lengths[i] > max_enc_length:\n",
        "            enc_truncated[i] = np.concatenate((enc_sentences[i,:max_enc_length - 100], enc_sentences[i,-100:]), axis = 0)\n",
        "            sentence_lengths[i] = max_enc_length    #truncated length of enc_sentences\n",
        "    #enc_labels = np.array(Labels)       #into numpy array\n",
        "    return enc_truncated, sentence_lengths\n",
        "\n",
        "\n",
        "class run():\n",
        "    def __init__(self, vocab,batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gradient_clip, kernel_size=0):\n",
        "        self.batch_size = batch_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.gradient_clip = gradient_clip\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.train_acc = []\n",
        "        self.valid_loss = []\n",
        "        self.valid_acc = []\n",
        "        self.count_matrix = np.zeros((4,4))\n",
        "\n",
        "        self.attention_dict = {}\n",
        "        self.data_att_dict = {}\n",
        "\n",
        "        #testing\n",
        "        self.prediction_score = 0\n",
        "        self.att_test = 0\n",
        "        self.vocab = vocab\n",
        "\n",
        "    \n",
        "    def setup_model(self, bidirection, if_attention, model_name):\n",
        "        if model_name == 'CNN-lstm':\n",
        "            self.model = CNN_lstm(self.batch_size, self.output_size, self.hidden_size, self.vocab_size, self.embedding_size, self.kernel_size, bidirection, if_attention)\n",
        "        else:\n",
        "            self.model = attentionLSTM(self.batch_size, self.output_size, self.hidden_size, self.vocab_size, self.embedding_size, bidirection, if_attention)\n",
        "        self.model = get_cuda(self.model)\n",
        "        self.optimiser = T.optim.Adam(self.model.parameters(), lr = self.lr)\n",
        "        pass\n",
        "    \n",
        "    def save_model(self, save_model_path, epoch):\n",
        "        save_path = save_model_path + \"/%05d.tar\"%(epoch+1)\n",
        "        T.save({\n",
        "            'epoch': epoch+1,\n",
        "            'model_dict': self.model.state_dict(),\n",
        "            'optimiser_dict': self.optimiser.state_dict()\n",
        "        }, save_path)\n",
        "\n",
        "    '''\n",
        "    def data_shuffle(self, enc_seq, enc_label, enc_length,random_seed = None):\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "        perm_rnd = np.random.permutation(len(enc_seq))\n",
        "        perm_seq = enc_seq[perm_rnd]\n",
        "        perm_label = enc_label[perm_rnd]\n",
        "        perm_length = enc_length[perm_rnd]\n",
        "\n",
        "        return perm_seq, perm_label, perm_length\n",
        "    '''\n",
        "    def one_batcher(self, seq, label, length, batch_index):\n",
        "        \"\"\"\n",
        "        params: batch_index: the index of current batch\n",
        "        \"\"\"\n",
        "        #if (batch_index+1) * self.batch_size > seq.shape[0]:        #excess the index of array\n",
        "        #    batch_data = seq[batch_index * self.batch_size :]\n",
        "        #    batch_label = label[batch_index * self.batch_size :]\n",
        "        #    batch_length = length[batch_index * self.batch_size :]\n",
        "        #else:\n",
        "        batch_data = seq[batch_indedx * self.batch_size : (batch_index + 1) * self.batch_size]\n",
        "        batch_label = label[batch_index * self.batch_size : (batch_index + 1) * self.batch_size]\n",
        "        batch_length = length[batch_index * self.batch_size : (batch_index + 1) * self.batch_size]\n",
        "        #fill in the padding array\n",
        "        enc_padding_mask = np.zeros(batch_data.shape, dtype = np.float32)\n",
        "        for row in range(batch_data.shape[0]):\n",
        "            enc_padding_mask[row, :batch_length[row]] = np.ones(batch_length[row], dtype = np.float32)\n",
        "           \n",
        "        return batch_data, batch_label, batch_length, enc_padding_mask\n",
        "\n",
        "\n",
        "    def train(self, enc_train, enc_train_label, enc_train_length, epoch):\n",
        "        \"\"\"\n",
        "        training for each epoch, first mini-batching, then train the model\n",
        "        params: enc_train: the encoded training sequence \n",
        "        params: enc_train_label: the encoded labels \n",
        "        params: enc_train_length: the corresponding length for the encoded sequence \n",
        "        \"\"\"\n",
        "\n",
        "        counter = 0\n",
        "        total_epoch_loss = 0\n",
        "        total_epoch_acc = 0\n",
        "        train_perm, train_label_perm, train_length_perm = data_shuffle(enc_train, enc_train_label, enc_train_length, random_seed = epoch+1)      #random shuffling the trainig data\n",
        "        #mini batching\n",
        "        N = enc_train.shape[0]\n",
        "        for n in range(N // self.batch_size):\n",
        "            batch_data, batch_label, _, enc_padding_mask = self.one_batcher(train_perm, train_label_perm, train_length_perm, batch_index = n)\n",
        "            input_data = T.LongTensor(batch_data)\n",
        "            input_label = T.LongTensor(batch_label)\n",
        "            input_padding_mask = T.LongTensor(enc_padding_mask)\n",
        "\n",
        "            input_data = get_cuda(input_data)\n",
        "            input_label = get_cuda(input_label)\n",
        "            input_padding_mask = get_cuda(input_padding_mask)\n",
        "\n",
        "            self.optimiser.zero_grad()\n",
        "            #print('size of input_data:', input_data.size())\n",
        "            prediction_score,_ = self.model(input_data, enc_padding_mask = input_padding_mask)        #return [batch_size, output_size] logits\n",
        "            loss = F.cross_entropy(prediction_score, input_label)\n",
        "\n",
        "            #training accuracy\n",
        "            num_correct = (T.max(prediction_score, 1)[1].view(input_label.size()).data == input_label.data).sum()\n",
        "            acc = 100.0 * num_correct/(len(batch_data))                \n",
        "\n",
        "            loss.backward()\n",
        "            if self.gradient_clip:\n",
        "                gradient_clamping(self.model, 1e-1)      #gradient clipping\n",
        "            self.optimiser.step()\n",
        "            counter+=1\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "        #list_train_loss.append(total_epoch_loss/counter)\n",
        "        #list_train_acc.append(total_epoch_acc/counter)\n",
        "        #print('After {} epoch, the training loss is {:.4f}; training accuracy is {:.2f}%'.format(i+1, total_epoch_loss/counter, total_epoch_acc/counter))\n",
        "        return total_epoch_loss/counter, total_epoch_acc/counter\n",
        "    \n",
        "    def validation(self, enc_valid, enc_valid_label, enc_valid_length, epoch, if_attention):\n",
        "        \"\"\"\n",
        "        validating the model for each epoch\n",
        "        \"\"\"\n",
        "        total_epoch_loss = 0\n",
        "        total_epoch_acc = 0\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            self.count_matrix = np.zeros((4,4))       #re-initialise error matrix\n",
        "\n",
        "        attention_dict = {'0':[], '1':[], '2':[], '3':[]}     #0: cyto; 1: secreted; 2: Mito; 3: nucleus\n",
        "        data_att_dict = {'0':[], '1':[], '2':[], '3':[]}\n",
        "\n",
        "        enc_valid, enc_valid_label, enc_valid_length = data_shuffle(enc_valid, enc_valid_label, enc_valid_length, random_seed = epoch+10)  #shuffle validation data\n",
        "        with T.no_grad():\n",
        "            N = enc_valid.shape[0]\n",
        "            #num_batch = N//self.batch_size if (N%self.batch_size==0) else (N//self.batch_size)+1\n",
        "            num_batch = N//self.batch_size\n",
        "            for n in range(num_batch):\n",
        "                batch_valid_data, batch_valid_label, _, enc_padding_mask_valid = self.one_batcher(enc_valid, enc_valid_label, enc_valid_length, \n",
        "                                                                                                                   batch_index = n)\n",
        "                input_valid_data = T.LongTensor(batch_valid_data)\n",
        "                input_valid_label = T.LongTensor(batch_valid_label)\n",
        "                input_valid_padding_mask = T.LongTensor(enc_padding_mask_valid)\n",
        "                \n",
        "                input_valid_data = get_cuda(input_valid_data)\n",
        "                input_valid_label = get_cuda(input_valid_label)\n",
        "                input_valid_padding_mask = get_cuda(input_valid_padding_mask)\n",
        "                                                                                                            #need to setup batch size as we might have leftover batches\n",
        "                prediction_valid_score, attention_weight= self.model(input_valid_data, enc_padding_mask = input_valid_padding_mask) #batch_size_setup = batch_valid_data.shape[0])\n",
        "                loss_valid = F.cross_entropy(prediction_valid_score, input_valid_label)\n",
        "                #validation accuracy\n",
        "                num_correct_valid = (T.max(prediction_valid_score, 1)[1].view(input_valid_label.size()).data == input_valid_label.data).sum()\n",
        "                acc_valid = 100.0 * num_correct_valid/(len(batch_valid_data))\n",
        "\n",
        "                #update attention matrix\n",
        "                if if_attention:\n",
        "                    if epoch%10 == 0:\n",
        "                        pred_label = T.max(prediction_valid_score, dim=1)[1]    #vector of predicted label\n",
        "                        for i in range(len(pred_label)):\n",
        "                            if int(pred_label[i]) == int(input_valid_label[i]):     #correct prediction\n",
        "                                attention_dict[str(int(pred_label[i]))].append(attention_weight[i].unsqueeze(0))\n",
        "\n",
        "                                #update data dictionary\n",
        "                                data_att_dict[str(int(pred_label[i]))].append(input_valid_data[i,:].unsqueeze(0))\n",
        "\n",
        "\n",
        "                        self.attention_dict = attention_dict\n",
        "                        self.data_att_dict = data_att_dict\n",
        "\n",
        "                #update error matrix\n",
        "                if epoch % 10 ==0:\n",
        "                    pred_temp = T.max(prediction_valid_score, 1)[1].view(input_valid_label.size()).data\n",
        "                    for i in range(input_valid_label.shape[0]):\n",
        "                        self.count_matrix[int(pred_temp[i]), int(input_valid_label.data[i])] += 1\n",
        "\n",
        "                total_epoch_loss += loss_valid\n",
        "                total_epoch_acc += acc_valid\n",
        "        return total_epoch_loss/num_batch, total_epoch_acc/num_batch\n",
        "\n",
        "    def lets_go(self, sentence_lengths, enc_truncated, enc_labels,k_fold, k,if_bidirection, if_attention, model_name,save_model_path=None):\n",
        "        '''\n",
        "        perform five-fold cross valdiation\n",
        "        params: sentence_length: array contains the truncated lengh of enc_sentences\n",
        "        params: enc_sentences: the encoded sequence with original length\n",
        "        params: Labels: array contains the label of enc_sentences\n",
        "        params: k_fold: K value of corss-validation\n",
        "        params: valud of kth fold\n",
        "        '''\n",
        "        self.setup_model(bidirection = if_bidirection, if_attention = if_attention, model_name = model_name)\n",
        "        #enc_labels = np.array(Labels)\n",
        "\n",
        "        val_size = len(sentence_lengths) // k_fold      #size of each segment\n",
        "        #start splitting the data\n",
        "        val_mask = np.arange(val_size * (k-1), val_size * k)        #kth fold\n",
        "        enc_valid = enc_truncated[val_mask]\n",
        "        enc_valid_label = enc_labels[val_mask]\n",
        "        enc_valid_length = sentence_lengths[val_mask]\n",
        "\n",
        "        enc_train = np.delete(enc_truncated, val_mask, axis = 0)\n",
        "        enc_train_label = np.delete(enc_labels, val_mask, axis = 0)\n",
        "        enc_train_length = np.delete(sentence_lengths, val_mask, axis = 0)\n",
        "        #keep the training and validation data fixed\n",
        "        for epoch in range(self.num_epoch):\n",
        "\n",
        "            epoch_train_loss, epoch_train_acc = self.train(enc_train, enc_train_label, enc_train_length, epoch)\n",
        "            epoch_valid_loss, epoch_valid_acc = self.validation(enc_valid, enc_valid_label, enc_valid_length, epoch, if_attention)\n",
        "\n",
        "            self.train_loss.append(epoch_train_loss)\n",
        "            self.train_acc.append(epoch_train_acc)\n",
        "            self.valid_loss.append(epoch_valid_loss)\n",
        "            self.valid_acc.append(epoch_valid_acc)\n",
        "            print(f'Epoch:{epoch+1 :02}, Train loss: {epoch_train_loss :.3f}, Train acc: {epoch_train_acc :.2f}%, Val loss: {epoch_valid_loss :.3f}, Val acc: {epoch_valid_acc :.2f}%')\n",
        "            \n",
        "            #if save_model_path is not None:\n",
        "            #    if (epoch+1)%5 == 0:\n",
        "            #        self.save_model(save_model_path, epoch)\n",
        "\n",
        "\n",
        "        return self.train_loss, self.train_acc, self.valid_loss, self.valid_acc, self.count_matrix\n",
        "\n",
        "\n",
        "        '''\n",
        "        #at each epoch, perform data random shuffling followed by K-fold cross-validation and train and validate on each combination\n",
        "        for epoch in range(self.num_epoch):\n",
        "            rnd_enc_seq, rnd_label, rnd_length = self.data_shuffle(enc_truncated, enc_labels, sentence_lengths, epoch+5)     #random shuffling\n",
        "            epoch_train_loss = 0\n",
        "            epoch_train_acc = 0\n",
        "            epoch_valid_loss = 0\n",
        "            epoch_valid_acc = 0\n",
        "\n",
        "            val_mask = np.arange(val_size * k, val_size * (k+1))        #[0:val]-->[val:2*val]-->[2*val:3*val]....\n",
        "\n",
        "            enc_valid = rnd_enc_seq[val_mask]                   #validation data splitting\n",
        "            enc_valid_label = rnd_label[val_mask]\n",
        "            enc_valid_length = rnd_length[val_mask]\n",
        "\n",
        "            enc_train = np.delete(rnd_enc_seq, val_mask, axis = 0)          #training data splitting\n",
        "            enc_train_label = np.delete(rnd_label, val_mask, axis = 0)\n",
        "                enc_train_length = np.delete(rnd_length, val_mask, axis = 0)\n",
        "\n",
        "                k_train_loss, k_train_acc = self.train(enc_train, enc_train_label, enc_train_length)         #perform training\n",
        "                k_valid_loss, k_valid_acc = self.validation(enc_valid, enc_valid_label, enc_valid_length)    #perform validating\n",
        "\n",
        "                epoch_train_loss += k_train_loss\n",
        "                epoch_train_acc += k_train_acc\n",
        "                epoch_valid_loss += k_valid_loss\n",
        "                epoch_valid_acc += k_valid_acc\n",
        "            \n",
        "            current_epoch_train_loss = epoch_train_loss/k_fold      #take the average\n",
        "            current_epoch_train_acc = epoch_train_acc/k_fold\n",
        "            current_epoch_valid_loss = epoch_valid_loss/k_fold\n",
        "            current_epoch_valid_acc = epoch_valid_acc/k_fold\n",
        "\n",
        "            print(f'Epoch:{epoch+1 :02}, Train loss: {current_epoch_train_loss :.3f}, Train acc: {current_epoch_train_acc :.2f}%, Val loss: {current_epoch_valid_loss :.3f}, Val acc: {current_epoch_valid_acc :.2f}%')\n",
        "            self.train_loss.append(current_epoch_train_loss)\n",
        "            self.train_acc.append(current_epoch_train_acc)\n",
        "            self.valid_loss.append(current_epoch_valid_loss)\n",
        "            self.valid_acc.append(current_epoch_valid_acc)\n",
        "\n",
        "            if save_model_path is not None:\n",
        "                if (epoch+1)%5 == 0:\n",
        "                    self.save_model(save_model_path, epoch)\n",
        "\n",
        "        return self.train_loss, self.train_acc, self.valid_loss, self.valid_acc\n",
        "        '''\n",
        "    def train_full_data(self, sentence_lengths, enc_truncated, enc_labels, if_bidirection, if_attention, model_name,save_model_path=None):\n",
        "        \"\"\"\n",
        "        performing training on full input data \n",
        "        \"\"\"\n",
        "        self.setup_model(bidirection = if_bidirection, if_attention = if_attention, model_name = model_name)\n",
        "        enc_train = enc_truncated\n",
        "        enc_train_label = enc_labels\n",
        "        enc_train_length = sentence_lengths\n",
        "\n",
        "        for epoch in range(self.num_epoch):\n",
        "            epoch_train_loss, epoch_train_acc = self.train(enc_train, enc_train_label, enc_train_length, epoch)\n",
        "\n",
        "            self.train_loss.append(epoch_train_loss)\n",
        "            self.train_acc.append(epoch_train_acc)\n",
        "            \n",
        "            print(f'Epoch:{epoch+1 :02}, Train loss: {epoch_train_loss :.3f}, Train acc: {epoch_train_acc :.2f}%.')\n",
        "\n",
        "        return self.train_loss, self.train_acc\n",
        "\n",
        "    def testing(self,testing_enc_seq, testing_length):\n",
        "        \"\"\"\n",
        "        perform testing, first padd the sequence if it has fewer samples than batch size\n",
        "        test_enc_seq:  array containing encoded testing sequence\n",
        "        testing_length:  array containing original length of the testin sequence\n",
        "        \"\"\"\n",
        "\n",
        "        N = testing_enc_seq.shape[0]\n",
        "\n",
        "        batch_test, test_padding_mask = self.test_padding(testing_enc_seq, testing_length)\n",
        "        assert batch_test.shape == test_padding_mask.shape == (self.batch_size, max_enc_length) \n",
        "\n",
        "        input_test = T.LongTensor(batch_test)\n",
        "        input_test_padding_mask = T.LongTensor(test_padding_mask)\n",
        "\n",
        "        input_test = get_cuda(input_test)\n",
        "        input_test_padding_mask = get_cuda(input_test_padding_mask)\n",
        "\n",
        "        pred, att_w = self.model(input_test, input_test_padding_mask)\n",
        "        self.prediction_score = pred[:N]            #discard the padding entry\n",
        "        self.att_test = att_w[:N]\n",
        "\n",
        "        return self.att_test            #outputing testing attention weight matrix \n",
        "\n",
        "    def test_padding(self, input_seq, input_length):\n",
        "        \"\"\"\n",
        "        pad the testing data as it has fewer samples than the batch size\n",
        "        input_seq:  encoded testing sequence\n",
        "        input_length:  the original length of testing sequence\n",
        "        \"\"\"\n",
        "            \n",
        "        padded = np.full((self.batch_size, input_seq.shape[1]), self.vocab['<PAD>'] ,dtype = np.float32)\n",
        "        padded[:input_seq.shape[0], :] = input_seq\n",
        "        enc_padding_mask = np.full(padded.shape, self.vocab['<PAD>'], dtype = np.float32)\n",
        "        for row in range(len(input_length)):    #20\n",
        "            enc_padding_mask[row, :input_length[row]] = np.ones(input_length[row], dtype = np.float32)\n",
        "        enc_padding_mask[len(input_length):, 1] = np.ones(self.batch_size - len(input_length), dtype = np.float32)\n",
        "        \n",
        "        return padded, enc_padding_mask\n",
        "\n",
        "    def print_frame(self):\n",
        "        \"\"\"\n",
        "        print the dataframe of predictive probability of each of four classes (testing)\n",
        "        \"\"\"\n",
        "        prob = self.prediction_score.detach().cpu().numpy()\n",
        "        prob_max = T.max(self.prediction_score, axis = 1)[1].cpu().numpy()\n",
        "        pred_class = np.array([])\n",
        "        for i in prob_max:\n",
        "            pred_class = np.append(pred_class, classes[i])\n",
        "\n",
        "        test_label_frame = []\n",
        "        for j in test['info']:\n",
        "            test_label_frame.append(j[1:-1])\n",
        "\n",
        "        df = pd.DataFrame(prob, index = test_label_frame,columns = classes)\n",
        "        df = df.round(3)\n",
        "        df['Prediction'] = pd.Series(pred_class, index = df.index)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    def plot(self, mode):\n",
        "        if mode == 'loss':\n",
        "            plt.plot(self.train_loss, label = 'training loss')\n",
        "            plt.plot(self.valid_loss, label = 'validation loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Loss plot')\n",
        "            plt.legend()\n",
        "\n",
        "        if mode == 'acc':\n",
        "            plt.plot(self.train_acc, label = 'training acc')\n",
        "            plt.plot(self.valid_acc, label = 'validation acc')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.title('Accuracy plot')\n",
        "            plt.legend()\n",
        "\n",
        "    def matrix_plot(self):\n",
        "        fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
        "\n",
        "        xlabel_list = [' ','cyto', ' ','secreted', ' ','mito', ' ','nucleus']\n",
        "        ylabel_list = [' ','cyto', ' ','secreted', ' ','mito', ' ','nucleus']\n",
        "        img = ax[0].imshow(self.count_matrix, aspect = 1, cmap = 'Blues')\n",
        "        ax[0].set_xticklabels(xlabel_list)\n",
        "        ax[0].set_yticklabels(ylabel_list)\n",
        "        ax[0].set_xlabel('prediction')\n",
        "        ax[0].set_ylabel('Ground truth')\n",
        "        ax[0].set_title('Matrix plot during validation')\n",
        "\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                c = self.count_matrix[j,i]\n",
        "                ax[0].text(i, j, str(int(c)), va='center', ha='center')\n",
        "        '''\n",
        "        temp_matrix = copy.copy(self.count_matrix)\n",
        "        np.fill_diagonal(temp_matrix, 0)\n",
        "        img2 = ax[1].imshow(temp_matrix, aspect = 1,cmap = 'Blues')\n",
        "        ax[1].set_xticklabels(xlabel_list)\n",
        "        ax[1].set_yticklabels(ylabel_list)\n",
        "        ax[1].set_xlabel('prediction')\n",
        "        ax[1].set_ylabel('Ground truth')\n",
        "        ax[1].set_title('Error during validation')   \n",
        "\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                c = temp_matrix[j,i]\n",
        "                ax[1].text(i, j, str(int(c)), va='center', ha='center')  \n",
        "        '''\n",
        "        row_sum = self.count_matrix.sum(axis = 0, keepdims = True)          #0 for precision, 1 for recall\n",
        "        error_rate = self.count_matrix/row_sum\n",
        "        img2 = ax[1].imshow(error_rate, aspect=1, cmap = 'Blues')\n",
        "        ax[1].set_xticklabels(xlabel_list)\n",
        "        ax[1].set_yticklabels(ylabel_list)\n",
        "        ax[1].set_xlabel('prediction')\n",
        "        ax[1].set_ylabel('Ground truth')\n",
        "        ax[1].set_title('Error during validation')        \n",
        "\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                c = error_rate[j,i]\n",
        "                ax[1].text(i,j, '{:.2f}%'.format(c*100), va = 'center', ha = 'center')\n",
        "        \n",
        "\n",
        "        fig.colorbar(img2,orientation='horizontal')\n",
        "        plt.tight_layout()\n",
        "    \n",
        "    def attention(self):\n",
        "        return self.attention_dict, self.data_att_dict\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CogKm82XWsSx",
        "colab_type": "text"
      },
      "source": [
        "#Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG74GPcmWXDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "perm_seq, perm_label,perm_length = data_shuffle(enc_sentences, np.array(Labels), sentence_lengths, random_seed = 12)\n",
        "enc_truncated, enc_length = truncate(perm_seq, perm_length)     #fixed for each model training\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6QzHNWbWzA4",
        "colab_type": "text"
      },
      "source": [
        "##LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdOimiXfWucK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32        #32,64,128,256\n",
        "output_size = 4\n",
        "hidden_size = 128        #128 for CNN-lstm without attention\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "kernel_size = [3, 5, 9]                 #for CNN+attention +LSTM the hidden size need to be lss\n",
        "model_name = 'lstm'\n",
        "num_epoch = 32\n",
        "k_fold = 5                  #k-fold value\n",
        "lr = 0.001\n",
        "bidirection = False\n",
        "if_attention = False\n",
        "gra_clip = True\n",
        "save_model_path = 'drive/My Drive/Bioinformatic/LSTM1_a'\n",
        "\n",
        "#perm_seq, perm_label,perm_length = data_shuffle(enc_sentences, np.array(Labels), sentence_lengths, random_seed = 123)\n",
        "#enc_truncated,# enc_length = truncate(perm_seq, perm_length)\n",
        "\n",
        "\n",
        "LSTM_1 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip, kernel_size)\n",
        "TL1, Tacc1, VL1, Vacc1, count_matrix = LSTM_1.lets_go(enc_length, enc_truncated, perm_label, k_fold,1, bidirection, if_attention, model_name,save_model_path)\n",
        "\n",
        "\n",
        "#Run_lstm0 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip)\n",
        "#TL0, Tacc0, VL0, Vacc0 = Run_lstm0.lets_go(sentence_lengths, enc_sentences, Labels, k_fold, bidirection, if_attention ,save_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmS4HFJPWue4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_1.matrix_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTR-sNUkW5E9",
        "colab_type": "text"
      },
      "source": [
        "##Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbHCKlSuW6os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32        #32,64,128,256\n",
        "output_size = 4\n",
        "hidden_size = 128        #128 for CNN-lstm without attention\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "kernel_size = [3, 5, 9]                 #for CNN+attention +LSTM the hidden size need to be lss\n",
        "model_name = 'lstm'\n",
        "num_epoch = 32\n",
        "k_fold = 5                  #k-fold value\n",
        "lr = 0.001\n",
        "bidirection = True\n",
        "if_attention = False\n",
        "gra_clip = True\n",
        "save_model_path = 'drive/My Drive/Bioinformatic/LSTM1_a'\n",
        "\n",
        "#perm_seq, perm_label,perm_length = data_shuffle(enc_sentences, np.array(Labels), sentence_lengths, random_seed = 123)\n",
        "#enc_truncated,# enc_length = truncate(perm_seq, perm_length)\n",
        "\n",
        "\n",
        "BLSTM = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip, kernel_size)\n",
        "TL2, Tacc2, VL2, Vacc2, count_matrix2 = BLSTM.lets_go(enc_length, enc_truncated, perm_label, k_fold,1, bidirection, if_attention, model_name,save_model_path)\n",
        "\n",
        "\n",
        "#Run_lstm0 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip)\n",
        "#TL0, Tacc0, VL0, Vacc0 = Run_lstm0.lets_go(sentence_lengths, enc_sentences, Labels, k_fold, bidirection, if_attention ,save_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzl5wb3vWuha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BLSTM.matrix_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrNCH7eXBBZ",
        "colab_type": "text"
      },
      "source": [
        "##LSTM-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehpbIf-kXC-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32        #32,64,128,256\n",
        "output_size = 4\n",
        "hidden_size = 128        #128 for CNN-lstm without attention\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "kernel_size = [3, 5, 9]                 #for CNN+attention +LSTM the hidden size need to be lss\n",
        "model_name = 'lstm'\n",
        "num_epoch = 22\n",
        "k_fold = 5                  #k-fold value\n",
        "lr = 0.001\n",
        "bidirection = False\n",
        "if_attention = True\n",
        "gra_clip = True\n",
        "save_model_path = 'drive/My Drive/Bioinformatic/LSTM1_a'\n",
        "\n",
        "#perm_seq, perm_label,perm_length = data_shuffle(enc_sentences, np.array(Labels), sentence_lengths, random_seed = 123)\n",
        "#enc_truncated,# enc_length = truncate(perm_seq, perm_length)\n",
        "\n",
        "\n",
        "LSTM_att = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip, kernel_size)\n",
        "TL_att, Tacc_att, VL_att, Vacc_att, count_matrix_att = LSTM_att.lets_go(enc_length, enc_truncated, perm_label, k_fold,1, bidirection, if_attention, model_name,save_model_path)\n",
        "\n",
        "\n",
        "#Run_lstm0 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip)\n",
        "#TL0, Tacc0, VL0, Vacc0 = Run_lstm0.lets_go(sentence_lengths, enc_sentences, Labels, k_fold, bidirection, if_attention ,save_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjHj_nC2XDDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "at, data_at = LSTM_att.attention()\n",
        "data_se = T.cat(data_at['1']).cpu().numpy()\n",
        "at_se = T.cat(at['1']).cpu().numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEiLI0MhXDGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_se = []\n",
        "for i in range(data_se.shape[0]):\n",
        "    temp = ''\n",
        "    for j in range(data_se.shape[1]):\n",
        "        if data_se[i,j] != 0:\n",
        "            temp += reverse_dictionary[data_se[i,j]]\n",
        "    seq_se.append(temp)\n",
        "seq_se"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu0C0CkqXlMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(at_se, aspect='auto', cmap='binary', vmin=0, vmax=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcYs2xOaXlzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "peptide = []\n",
        "threshold = 0.05\n",
        "for i in range(at_se.shape[0]):\n",
        "    temp = ''\n",
        "    for j in range(at_se.shape[1]):\n",
        "        if at_se[i,j] == 0:\n",
        "            break\n",
        "        if at_se[i,j] >threshold:\n",
        "            temp += seq_se[i][j]\n",
        "        else:\n",
        "            temp += '_'\n",
        "    peptide.append(temp)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYrlMsSWXl3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(peptide)):\n",
        "    print(seq_se[i] + '\\n' + peptide[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FwykLQoXl5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = 3\n",
        "fig, ax = plt.subplots(2,2,figsize=(15,15))\n",
        "\n",
        "\n",
        "ax[0,0].plot(at_se[t][:len(seq_se[t])], label='Secreted protein')\n",
        "for i,txt in enumerate(seq_se[t]):\n",
        "    ax[0,0].annotate(txt, (i, at_se[t][i]),fontsize='xx-large')\n",
        "ax[0,0].set_xlabel('position')\n",
        "ax[0,0].set_ylabel('attention weight');\n",
        "ax[0,0].legend()\n",
        "\n",
        "t2 = 6\n",
        "ax[0,1].plot(at_se[t2][:len(seq_se[t2])], label='Secreted protein')\n",
        "for i,txt in enumerate(seq_se[t2]):\n",
        "    ax[0,1].annotate(txt, (i, at_se[t2][i]),fontsize='xx-large')\n",
        "ax[0,1].set_xlabel('position')\n",
        "ax[0,1].set_ylabel('attention weight');\n",
        "ax[0,1].legend()\n",
        "\n",
        "t3 = 8\n",
        "ax[1,0].plot(at_se[t3][:len(seq_se[t3])], label='Secreted protein')\n",
        "for i,txt in enumerate(seq_se[t3]):\n",
        "    ax[1,0].annotate(txt, (i, at_se[t3][i]),fontsize='xx-large')\n",
        "ax[1,0].set_xlabel('position')\n",
        "ax[1,0].set_ylabel('attention weight');\n",
        "ax[1,0].legend()\n",
        "\n",
        "t4 = 203\n",
        "ax[1,1].plot(at_se[t4][:len(seq_se[t4])], label='Secreted protein')\n",
        "for i,txt in enumerate(seq_se[t4]):\n",
        "    ax[1,1].annotate(txt, (i, at_se[t4][i]),fontsize='xx-large')\n",
        "ax[1,1].set_xlabel('position')\n",
        "ax[1,1].set_ylabel('attention weight');\n",
        "ax[1,1].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWYFaqSvYE_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppTPioIHYFCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#attention plot\n",
        "\n",
        "at_secreted = T.cat(at['1'], dim=0).cpu().numpy()\n",
        "at_secreted\n",
        "\n",
        "adjust_se = np.zeros(at_secreted.shape)\n",
        "\n",
        "for i in range(at_secreted.shape[0]):\n",
        "    num_nonzero = np.count_nonzero(at_secreted[i])\n",
        "    if num_nonzero <2000:\n",
        "        adjust_se[i,: int(num_nonzero/2)] = at_secreted[i, : int(num_nonzero/2)]\n",
        "        adjust_se[i,-(num_nonzero - int(num_nonzero/2)):] = at_secreted[i,int(num_nonzero/2):num_nonzero]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hc3WdG-YFFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "#img = ax[0].imshow(adjust, interpolation='nearest',aspect='auto', cmap = 'binary', vmin=0, vmax=0.03);\n",
        "img2 = ax.imshow(adjust_se, interpolation='nearest', aspect='auto', cmap = 'binary', vmin = 0, vmax = 0.03);\n",
        "ax.set_title('Secreted attention')\n",
        "ax.set_xlabel('Position')\n",
        "ax.set_ylabel('Sequences number')\n",
        "fig.colorbar(img2,orientation='vertical')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4mcNF1fYaY2",
        "colab_type": "text"
      },
      "source": [
        "#CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNsKyJptYbwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64        #32,64,128,256\n",
        "output_size = 4\n",
        "hidden_size = 128\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "kernel_size = [15, 21, 27]\n",
        "model_name = 'CNN-lstm'\n",
        "num_epoch = 100\n",
        "k_fold = 5\n",
        "lr = 0.001\n",
        "bidirection = False\n",
        "if_attention = False\n",
        "gra_clip = True\n",
        "save_model_path = 'drive/My Drive/Bioinformatic/LSTM1_a'\n",
        "\n",
        "#perm_seq, perm_label,perm_length = data_shuffle(enc_sentences, np.array(Labels), sentence_lengths, random_seed = 123)\n",
        "#enc_truncated,# enc_length = truncate(perm_seq, perm_length)\n",
        "\n",
        "\n",
        "LSTM_cnn = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip, kernel_size)\n",
        "TL4, Tacc4, VL4, Vacc4, count_matrix4 = LSTM_cnn.lets_go(enc_length, enc_truncated, perm_label, k_fold,1, bidirection, if_attention, model_name,save_model_path)\n",
        "\n",
        "\n",
        "#Run_lstm0 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip)\n",
        "#TL0, Tacc0, VL0, Vacc0 = Run_lstm0.lets_go(sentence_lengths, enc_sentences, Labels, k_fold, bidirection, if_attention ,save_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOxw7-f0YlC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_cnn.matrix_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFeSpU7VYniT",
        "colab_type": "text"
      },
      "source": [
        "##5-folds cross validation using LSTM with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2sTDm-mYuxF",
        "colab_type": "text"
      },
      "source": [
        "###fold-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFqKPPvyYyOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32        #32,64,128,256\n",
        "output_size = 4\n",
        "hidden_size = 128        #128 for CNN-lstm without attention\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "kernel_size = [3, 5, 9]                 #for CNN+attention +LSTM the hidden size need to be lss\n",
        "model_name = 'lstm'\n",
        "num_epoch = 60\n",
        "k_fold = 5                  #k-fold value\n",
        "lr = 0.001\n",
        "bidirection = False\n",
        "if_attention = True\n",
        "gra_clip = True\n",
        "save_model_path = 'drive/My Drive/Bioinformatic/LSTM1_a'\n",
        "\n",
        "#perm_seq, perm_label,perm_length = data_shuffle(enc_sentences, np.array(Labels), sentence_lengths, random_seed = 123)\n",
        "#enc_truncated,# enc_length = truncate(perm_seq, perm_length)\n",
        "\n",
        "\n",
        "att1 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip, kernel_size)\n",
        "TL1, Tacc1, VL1, Vacc1, count_matrix1 = att1.lets_go(enc_length, enc_truncated, perm_label, k_fold,1, bidirection, if_attention, model_name,save_model_path)\n",
        "                                                                                                #fold 1!\n",
        "\n",
        "#Run_lstm0 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip)\n",
        "#TL0, Tacc0, VL0, Vacc0 = Run_lstm0.lets_go(sentence_lengths, enc_sentences, Labels, k_fold, bidirection, if_attention ,save_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WgrDl4EY2Dk",
        "colab_type": "text"
      },
      "source": [
        "..........."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGRpVWdfY3b8",
        "colab_type": "text"
      },
      "source": [
        "###train on full data and perform testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFzSaHE6ZEOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32        #32,64,128,256\n",
        "output_size = 4\n",
        "hidden_size = 128        #128 for CNN-lstm without attention\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "kernel_size = [3, 5, 9]                 #for CNN+attention +LSTM the hidden size need to be lss\n",
        "model_name = 'lstm'\n",
        "num_epoch = 20\n",
        "#k_fold = 5                  #k-fold value\n",
        "lr = 0.001\n",
        "bidirection = False\n",
        "if_attention = True\n",
        "gra_clip = True\n",
        "save_model_path = 'drive/My Drive/Bioinformatic/LSTM1_a'\n",
        "\n",
        "#perm_seq, perm_label,perm_length = data_shuffle(enc_sentences, np.array(Labels), sentence_lengths, random_seed = 123)\n",
        "#enc_truncated,# enc_length = truncate(perm_seq, perm_length)\n",
        "\n",
        "\n",
        "full_run = run(vocab,batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip, kernel_size)\n",
        "TL_full, Tacc_full = full_run.train_full_data(enc_length, enc_truncated, perm_label, bidirection, if_attention, model_name,save_model_path)\n",
        "\n",
        "\n",
        "#Run_lstm0 = run(batch_size, output_size, hidden_size, vocab_size, embedding_size, num_epoch, lr, gra_clip)\n",
        "#TL0, Tacc0, VL0, Vacc0 = Run_lstm0.lets_go(sentence_lengths, enc_sentences, Labels, k_fold, bidirection, if_attention ,save_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ9VejyDZI-a",
        "colab_type": "text"
      },
      "source": [
        "testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfPBN9MaZIXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_test = test_enc_sentences\n",
        "enc_test_length = test_sentence_lengths\n",
        "at = full_run.testing(enc_test, enc_test_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQ7Hg4dZMMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_run.print_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlHWrIe0ZaI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention = at.cpu().detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFqoQntBZdvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adjust_test = np.zeros(attention.shape)\n",
        "\n",
        "for i in range(attention.shape[0]):\n",
        "    num_nonzero = np.count_nonzero(attention[i])\n",
        "    if num_nonzero <2000:\n",
        "        adjust_test[i,: int(num_nonzero/2)] = attention[i, : int(num_nonzero/2)]\n",
        "        adjust_test[i,-(num_nonzero - int(num_nonzero/2)):] = attention[i,int(num_nonzero/2):num_nonzero]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roed7LvhZe5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "img = ax.imshow(adjust_test, interpolation='nearest',aspect='auto', cmap = 'binary', vmin=0, vmax=0.03);\n",
        "#img2 = ax.imshow(adjust_se, interpolation='nearest', aspect='auto', cmap = 'binary', vmin = 0, vmax = 0.03);\n",
        "ax.set_title('testing attention')\n",
        "ax.set_xlabel('Position')\n",
        "ax.set_ylabel('Sequences number')\n",
        "fig.colorbar(img,orientation='vertical')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiruC51aZgOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "peptide = []\n",
        "threshold = 0.01\n",
        "for i in range(attention.shape[0]):\n",
        "    temp = ''\n",
        "    for j in range(attention.shape[1]):\n",
        "        if attention[i,j] == 0:\n",
        "            break\n",
        "        if attention[i,j] >threshold:\n",
        "            temp += test['seq'][i][j]\n",
        "        else:\n",
        "            temp += '_'\n",
        "    peptide.append(temp)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQycSnjpZhlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(peptide)):\n",
        "    print(test['seq'][i] + '\\n' + peptide[i])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}